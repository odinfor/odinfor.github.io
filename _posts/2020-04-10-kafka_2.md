---
layout: post
title: "「中间件」kafka学习笔记(二)"
subtitle: 'kafka可靠性&唯一性，以及消息丢失和重复场景分析'
author: "odin"
header-style: text
tags:
  - 中间件
---

### 1.Producer可靠性策略
选举机制类似zookeeper的zab一致性算法，也是基于大多数原则，此处不深入介绍。略显不同的是kafka中有controller的概念，所有的broker向zookeeper中抢注/controller这个临时（EPHEMERAL）节点，第一个抢注成功的broker成为controller，在具备broker的责任同时管理kafka所有broker、partition以及replica变化。

/controller临时节点内容大致如下
```
{"version":1,"brokerid":0,"timestamp":"1529210278988"}
```
Producer可靠性策略
acks：此配置是 Producer 在确认一个请求发送完成之前需要收到的反馈信息的数量。 这个参数是为了保证发送请求的可靠性。以下配置方式是允许的：
acks=0，设置为0时producer发送消息后不会等待kafka服务器的确认返回。该消息会被立刻添加到 socket buffer 中并认为已经发送完成。在这种情况下，服务器是否收到请求是没法保证的，并且参数retries也不会生效（因为客户端无法获得失败信息）。每个记录返回的 offset 总是被设置为-1。
acks=1，设置为1时producer发送消息后会leader节点将日志写入本地成功后不等待follower节点的确认反馈就给producer返回确认消息。该设置下回出现消息丢失。

acks=-1，设置为-1时(即all)，发送消息后，leader节点会等待所有follower返回确认消息后才返回给producer确认消息。该设置下会出现消息重复。

以上三种策略需要根据应用场景评估。

消息丢失现象分析：当acks=1时，leader本地写入日志成功，返回给producer确认消息后，在与任意一follower节点同步过程均未完成时，leader节点挂掉，那么重新选举后，由于原follower节点中没有同步该条数据，而producer中确认该条消息发送成功，于是出现消息丢失。

消息重复消费分析：当acks=-1时，leader在与follower同步过程中挂掉，部分follower节点同步完成，而部分follower节点未完成，重新选举后会触发消息重新发送，导致该消息重复消费。

## 2.Producer工作流程
### 2.1）基本工作流程
kafka的producer发送消息采用的是异步发送方式。既然是异步那么至少包含2个线程。在producer的发送消息过程中使用了一个main线程和一个sender线程，另外还有一个线程`RecordAccumulator`(消息缓冲池)。
* `main`线程：将消息发送给`RecordAccumulator`。
* `sender`线程：不断的从`RecordAccumulator`拉取消息发送到`broker`。

![]({{site.baseurl}}/img/in-post/post-middleware/kafka_producer_process.jpg)

producer工作流程：
1. 将待发送的数据封装成`ProducerRecord Object`。
2. `ProducerRecord Object`经过拦截器`Intercrptors`。
3. `ProducerRecord Object`经过序列化器`serializer`。
4. `ProducerRecord Object`经过分区器`partitioner`。
5. `ProducerRecord Object`发送到缓冲池`RecordAccumulator`。
6. sender线程不断轮询，从缓存池中拉取发送消息数据进行发送。

### 2.2）RecordAccumulatorkafka作用
kafka为了减少频繁发送消息建立tcp的消耗，每次发送一批消息数据,由batch.size控制。基于这个规则，kafka优先将每条消费消息发送至RecordAccumulator缓存起来，由sender线程每隔一段时间从缓存池中拉取一定大小的数据。间隔时间由linger.ms配置控制。

官方对linger.ms和batch.size给出的说明
linger.ms：producer 会将两个请求发送时间间隔内到达的记录合并到一个单独的批处理请求中。通常只有当记录到达的速度超过了发送的速度时才会出现这种情况。然而，在某些场景下，即使处于可接受的负载下，客户端也希望能减少请求的数量。这个设置是通过添加少量的人为延迟来实现的&mdash；即，与其立即发送记录， producer 将等待给定的延迟时间，以便将在等待过程中到达的其他记录能合并到本批次的处理中。这可以认为是与 TCP 中的 Nagle 算法类似。这个设置为批处理的延迟提供了上限:一旦我们接受到记录超过了分区的 batch.size ，Producer 会忽略这个参数，立刻发送数据。但是如果累积的字节数少于 batch.size ，那么我们将在指定的时间内“逗留”(linger)，以等待更多的记录出现。这个设置默认为0(即没有延迟)。例如：如果设置linger.ms=5 ，则发送的请求会减少并降低部分负载，但同时会增加5毫秒的延迟。

batch.size：当将多个记录被发送到同一个分区时， Producer 将尝试将记录组合到更少的请求中。这有助于提升客户端和服务器端的性能。这个配置控制一个批次的默认大小（以字节为单位）。当记录的大小超过了配置的字节数， Producer 将不再尝试往批次增加记录。发送到 broker 的请求会包含多个批次的数据，每个批次对应一个 partition 的可用数据小的 batch.size 将减少批处理，并且可能会降低吞吐量(如果 batch.size = 0的话将完全禁用批处理)。 很大的 batch.size 可能造成内存浪费，因为我们一般会在 batch.size 的基础上分配一部分缓存以应付额外的记录。

* 若linger.ms到达，缓存池中数据超过一个batch.size大小，则sender拉取一个batch.size的数据发送给broker。
* 若linger.ms到达，缓存池中数据不超过一个batch.size大小，本次不拉取数据。
* 若再次到达linger.ms，缓存池中数据依然不超过一个batch.size，那么本次sender拉取全部数据发送给broker。